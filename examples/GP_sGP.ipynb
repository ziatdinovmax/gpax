{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ziatdinovmax/gpax/blob/main/examples/GP_sGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For github continuous integration only\n",
    "# Please ignore if you're running this notebook!\n",
    "import os\n",
    "if os.environ.get(\"CI_SMOKE\"):\n",
    "    NUM_WARMUP = 100\n",
    "    NUM_SAMPLES = 100\n",
    "else:\n",
    "    NUM_WARMUP = 2000\n",
    "    NUM_SAMPLES = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtiDY7RcohrR"
   },
   "source": [
    "# Structured Gaussian Process\n",
    "\n",
    "This notebook compares vanilla and structured Gaussian processes for reconstructing and active learning of function characterized by a discontinuous behavior at some \"transition\" point.\n",
    "\n",
    "*Prepared by Maxim Ziatdinov (2022). Last updated in October 2023.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUoeZ9Tin74Q"
   },
   "source": [
    "In the previous [example](https://colab.research.google.com/github/ziatdinovmax/gpax/blob/main/examples/simpleGP.ipynb), we have introduced Gaussian process (GP) operating in a fully Bayesian mode for reconstucting, with quantified uncertainty, an unknown function from sparse measurements. The limitation of the standard GP is that it does not usually allow for the incorporation of prior domain knowledge and can be biased toward a trivial interpolative solution. Recently, we introduced a structured Gaussian Process (sGP), where a classical GP is augmented by a structured probabilistic model of the expected system‚Äôs behavior. This approach allows us to balance the flexibility of the non-parametric GP approach with a rigid structure of prior (physical) knowledge encoded into the parametric model. Implementation-wise, this is achieved by substituting a zero prior mean function in GP with a probabilistic model of the expected system's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdtH0tCPQ2de"
   },
   "source": [
    "## Install & Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86iUwKxLO7qE"
   },
   "source": [
    "Install the latest GPax package from PyPI (this is best practice, as it installs the latest, deployed and tested version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ1rLUzqha2i",
    "outputId": "44157aab-4e21-4966-ec79-ccf85cd4bbaa"
   },
   "outputs": [],
   "source": [
    "!pip install gpax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vygoK7MTjJWB"
   },
   "source": [
    "Import needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For use on Google Colab\n",
    "    import gpax\n",
    "\n",
    "except ImportError:\n",
    "    # For use locally (where you're using the local version of gpax)\n",
    "    print(\"Assuming notebook is being run locally, attempting to import local gpax module\")\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "    import gpax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtGDc11Ehh7r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpax.utils.enable_x64()  # enable double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable some pretty plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSaWCRHJPukv"
   },
   "source": [
    "## Standard and structured GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6vVHZKpxse_"
   },
   "source": [
    "We consider noisy observations of a discontinuous function... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise1(x: np.ndarray, params) -> np.ndarray:\n",
    "    return np.piecewise(\n",
    "        x,\n",
    "        [x < params[\"t\"], x >= params[\"t\"]],\n",
    "        [lambda x: x**params[\"beta1\"], lambda x: x**params[\"beta2\"]])\n",
    "\n",
    "\n",
    "NUM_INIT_POINTS = 15 # number of observation points\n",
    "NOISE_LEVEL = 0.1\n",
    "PARAMS = {\"t\": 1.7, \"beta1\": 4.5, \"beta2\": 2.5}\n",
    "\n",
    "np.random.seed(1)\n",
    "X = np.random.uniform(0, 3, NUM_INIT_POINTS)\n",
    "y = piecewise1(X, PARAMS) + np.random.normal(0., NOISE_LEVEL, NUM_INIT_POINTS)\n",
    "\n",
    "X_test = np.linspace(0, 3, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "LF2l_UcBtaDT",
    "outputId": "4736900d-f587-449b-b37d-609bc98ed028"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(6, 2))            \n",
    "ax.scatter(X, y, alpha=0.5, c='k', marker='x', label=\"Noisy observations\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_xlim(0, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPfkj1cRQLop"
   },
   "source": [
    "... and try to reconstruct this underlying function with a standard GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "cOtfzDCo0MMI",
    "outputId": "96e6d7f2-832d-4c00-b273-c0910260acfc"
   },
   "outputs": [],
   "source": [
    "# Get random number generator keys (see JAX documentation for why it is neccessary)\n",
    "rng_key, rng_key_predict = gpax.utils.get_keys()\n",
    "\n",
    "# Initialize model\n",
    "gp_model = gpax.ExactGP(1, kernel='Matern')\n",
    "\n",
    "# Run HMC to obtain posterior samples\n",
    "gp_model.fit(rng_key, X, y, num_chains=1)\n",
    "\n",
    "# Get GP prediction\n",
    "y_pred, y_sampled = gp_model.predict(rng_key_predict, X_test, n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2))\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.scatter(X, y, marker='x', c='k', zorder=1, label=\"Noisy observations\", alpha=0.7)\n",
    "for y1 in y_sampled:\n",
    "    ax.plot(X_test, y1.mean(0), lw=.1, zorder=0, c='r', alpha=.1)\n",
    "l, = ax.plot(X_test, y_sampled[0].mean(0), lw=1, c='r', alpha=1, label=\"Sampled predictions\")\n",
    "ax.plot(X_test, y_pred, lw=1.5, zorder=1, c='b', label='Sampled means (CoM)')\n",
    "ax.plot(X_test, piecewise1(X_test, PARAMS), c='k', linestyle='--', label='True function', alpha=0.5)\n",
    "ax.legend(loc='upper left')\n",
    "l.set_alpha(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxQiSvDv3Ur0"
   },
   "source": [
    "The standard GP did not perform very well. Now let's try GP augmented by a probabilistic models of *expected* system's behavior. We'l need to use JAX's version of numpy for defining operations on arrays and NumPyro for placing priors over model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjxPG_gY3U2c"
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpyro\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ31U3oPhe8u"
   },
   "source": [
    "Define possible models as deterministic functions. Notice that the first model has a correct 'guess' about the underlying function. The second model is only partially correct (it assumes the existence of transition point, but describe the behavior before and after that point as linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdrtXqGPKzUe"
   },
   "outputs": [],
   "source": [
    "def piecewise1(x: jnp.ndarray, params: Dict[str, float]) -> jnp.ndarray:\n",
    "    \"\"\"Power-law behavior before and after the transition\"\"\"\n",
    "    return jnp.piecewise(\n",
    "        x, [x < params[\"t\"], x >= params[\"t\"]],\n",
    "        [lambda x: x**params[\"beta1\"], lambda x: x**params[\"beta2\"]])\n",
    "\n",
    "def piecewise2(x: jnp.ndarray, params: Dict[str, float]) -> jnp.ndarray:\n",
    "    \"\"\"Linear behavior before and after the transition\"\"\"\n",
    "    return jnp.piecewise(\n",
    "        x, [x < params[\"t\"], x >= params[\"t\"]],\n",
    "        [lambda x: params[\"b\"]*x, lambda x: params[\"c\"]*x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6az3H14qKQg"
   },
   "source": [
    "Put priors over parameters of each model (to make them probabilistic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqXxUSGeqGhm"
   },
   "outputs": [],
   "source": [
    "def piecewise1_priors():\n",
    "    # Sample model parameters\n",
    "    t = numpyro.sample(\"t\", numpyro.distributions.Uniform(0.5, 2.5))\n",
    "    beta1 = numpyro.sample(\"beta1\", numpyro.distributions.LogNormal(0, 1))\n",
    "    beta2 = numpyro.sample(\"beta2\", numpyro.distributions.LogNormal(0, 1))\n",
    "    # Return sampled parameters as a dictionary\n",
    "    return {\"t\": t, \"beta1\": beta1, \"beta2\": beta2}\n",
    "\n",
    "\n",
    "def piecewise2_priors():\n",
    "    # Sample model parameters\n",
    "    t = numpyro.sample(\"t\", numpyro.distributions.Uniform(0.5, 2.5))\n",
    "    b = numpyro.sample(\"b\", numpyro.distributions.LogNormal(0, 1))\n",
    "    c = numpyro.sample(\"c\", numpyro.distributions.LogNormal(0, 1))\n",
    "    # Return sampled parameters as a dictionary\n",
    "    return {\"t\": t, \"b\": b, \"c\": c}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2VOKHNrUn-A"
   },
   "source": [
    "Run a 'structured' GP (*s*GP) for each model. Note that to make our GP 'structured', we pass the ```mean_fn``` (deterministic function) and ```mean_fn_prior``` (priors over the function parameters) arguments to it at the initialization stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1CunEclMqnFW",
    "outputId": "1491e0c5-98af-45f5-a9c1-fe8f22c0ff7a"
   },
   "outputs": [],
   "source": [
    "mean_fn = [piecewise1, piecewise2]\n",
    "mean_fn_priors = [piecewise1_priors, piecewise2_priors]\n",
    "\n",
    "for m, mp in zip(mean_fn, mean_fn_priors):\n",
    "    \n",
    "    # Initialize model\n",
    "    gp_model = gpax.ExactGP(1, kernel='Matern', mean_fn=m, mean_fn_prior=mp)\n",
    "    \n",
    "    # Run MCMC to obtain posterior samples\n",
    "    gp_model.fit(rng_key, X, y, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    \n",
    "    # Get GP prediction\n",
    "    y_pred, y_sampled = gp_model.predict(rng_key_predict, X_test, n=200)\n",
    "    \n",
    "    # Plot results\n",
    "    _, ax = plt.subplots(dpi=100)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.scatter(X, y, marker='x', c='k', zorder=1, label=\"Noisy observations\", alpha=0.7)\n",
    "    for y1 in y_sampled:\n",
    "        ax.plot(X_test, y1.mean(0), lw=.1, zorder=0, c='r', alpha=.1)\n",
    "    l, = ax.plot(X_test, y_sampled[0].mean(0), lw=1, c='r', alpha=1, label=\"Sampled predictions\")\n",
    "    ax.plot(X_test, y_pred, lw=1.5, zorder=1, c='b', label='Sampled means (CoM)')\n",
    "    ax.plot(X_test, piecewise1(X_test, PARAMS), c='k', linestyle='--', label='True function', alpha=0.5)\n",
    "    ax.legend(loc='upper left')\n",
    "    l.set_alpha(0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vxIlg5vU41N"
   },
   "source": [
    "We can see that the first *s*GP model performed exceptionally well, except for the transition region where we do not have enough observations. However, this region is also characterized by very large uncertainty (variation in the sampled predictions), suggesting that one may want to perform extra measurements in that area. For the second *s*GP model, the reconstruction quality is much lower, even though it is still somewhat better than for vanilla GP. This is not surprising because the second model is much less acccurate (it assumes linear behavior before and after the transition point, which is obviously not the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OcE0vARnynx"
   },
   "source": [
    "## Simple active learning with structured GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8N2CbVk3vlP"
   },
   "source": [
    "We can further explore regions with high uncertainty using the active learning approach. In this approach, the next evaluation point is selected according to $$x_{next}=\\arg \\max_xùúé[f_*]$$ \n",
    "\n",
    "where $ùúé$ is the uncertainty in prediction of function $f_*$ over the parameter range $x$. First, we are going to use the model that produced the lowest total uncertainty on the original set of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRocZMUIVsp4"
   },
   "outputs": [],
   "source": [
    "# Copy the initial observations so that we can re-use them later\n",
    "Xo, yo = X.copy(), y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niggsunegVLQ",
    "outputId": "d4e9464f-42f7-4cc9-f599-3906ec13722b"
   },
   "outputs": [],
   "source": [
    "rng_key, rng_key_predict = gpax.utils.get_keys(1)\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"\\nExploration step {}\".format(i+1))\n",
    "    # Obtain/update GP posterior\n",
    "    gp_model = gpax.ExactGP(1, kernel='Matern', mean_fn=piecewise1, mean_fn_prior=piecewise1_priors)\n",
    "    gp_model.fit(rng_key, X, y, print_summary=1, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    # Compute acquisition function (here it is simply the uncertinty in prediciton)\n",
    "    # and get the coordinate of the next point to measure\n",
    "    obj = gpax.acquisition.UE(rng_key_predict, gp_model, X_test)\n",
    "    next_point_idx = obj.argmax()\n",
    "     # Append the 'suggested' point\n",
    "    X = np.append(X, X_test[next_point_idx])\n",
    "    measured = piecewise1(X_test[next_point_idx], PARAMS) + np.random.normal(0., NOISE_LEVEL)  # we assume that new observations are also noisy\n",
    "    y = np.append(y, measured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAFmB-A3xW03"
   },
   "source": [
    "Make the prediction using all the newly discovered points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJuaJasbki95",
    "outputId": "c99404e6-0bca-461d-90ec-d9504607f847"
   },
   "outputs": [],
   "source": [
    "rng_key, rng_key_predict = gpax.utils.get_keys(1)\n",
    "# Update GP posterior\n",
    "gp_model = gpax.ExactGP(1, kernel='Matern', mean_fn=piecewise1, mean_fn_prior=piecewise1_priors)\n",
    "gp_model.fit(rng_key, X, y)\n",
    "# Get GP prediction\n",
    "y_pred, y_sampled = gp_model.predict(rng_key_predict, X_test, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "U_kaGIxzEUey",
    "outputId": "67f7216c-f0aa-46f9-b487-73fa4f04577a"
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "truefunc = piecewise1(X_test, PARAMS)\n",
    "seed_points = 15\n",
    "plt.figure(dpi=100)\n",
    "plt.scatter(X[seed_points:], y[seed_points:], c=jnp.arange(1, len(X[seed_points:])+1),\n",
    "            cmap='viridis', label=\"Sampled points\", zorder=1)\n",
    "cbar = plt.colorbar(label=\"Exploration step\")\n",
    "cbar_ticks = np.arange(2, len(X[seed_points:]) + 1, 2)\n",
    "cbar.set_ticks(cbar_ticks)\n",
    "plt.scatter(X[:seed_points], y[:seed_points], marker='x', s=64,\n",
    "            c='k', label=\"Seed points\", zorder=1)\n",
    "plt.plot(X_test, truefunc, c='k', label='True function', zorder=0)\n",
    "plt.plot(X_test, y_pred, '--', c='red', label='Model reconstruction', zorder=0)\n",
    "\n",
    "plt.fill_between(X_test, y_pred - y_sampled.std((0,1)), y_pred + y_sampled.std((0,1)),\n",
    "                        color='r', alpha=0.2, label=\"Model uncertainty\", zorder=0)\n",
    "plt.xlabel(\"$x$\", fontsize=12)\n",
    "plt.ylabel(\"$y$\", fontsize=12)\n",
    "plt.legend(fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_41qn3YXCly"
   },
   "source": [
    "Now we are going do active learning with the 'wrong' (or partially correct, depending on whether you are pessimist or optimist) model. We will start from the same set of observations as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIS_fPUSW60b",
    "outputId": "c6a4f3b0-3016-4006-80f4-4a2c31e6e6b4"
   },
   "outputs": [],
   "source": [
    "X, y = Xo, yo  # start from the same set of observations\n",
    "\n",
    "rng_key, rng_key_predict = gpax.utils.get_keys(1)\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"\\nExploration step {}\".format(i+1))\n",
    "    # Obtain/update GP posterior\n",
    "    gp_model = gpax.ExactGP(1, kernel='Matern', mean_fn=piecewise2, mean_fn_prior=piecewise2_priors)\n",
    "    gp_model.fit(rng_key, X, y, print_summary=1, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    # Compute acquisition function and get coordinate of the next point\n",
    "    obj = gpax.acquisition.UE(rng_key_predict, gp_model, X_test)\n",
    "    next_point_idx = obj.argmax()\n",
    "     # Append the 'suggested' point\n",
    "    X = np.append(X, X_test[next_point_idx])\n",
    "    measured = piecewise1(X_test[next_point_idx], PARAMS) + np.random.normal(0., NOISE_LEVEL)  # we assume that new observations are also noisy\n",
    "    y = np.append(y, measured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXVeOE0FW60d"
   },
   "source": [
    "Make the prediction using all the newly discovered points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a76Tly_yW60e",
    "outputId": "bb5c3858-f5e6-4b1f-daa3-2fa9aed38f72"
   },
   "outputs": [],
   "source": [
    "rng_key, rng_key_predict = gpax.utils.get_keys(1)\n",
    "# Update GP posterior\n",
    "gp_model = gpax.ExactGP(1, kernel='Matern', mean_fn=piecewise2, mean_fn_prior=piecewise2_priors)\n",
    "gp_model.fit(rng_key, X, y, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "# Get GP prediction\n",
    "y_pred, y_sampled = gp_model.predict(rng_key_predict, X_test, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "ciIpQFqCW60e",
    "outputId": "664ea029-65e3-4a2f-ed35-57b9e42c8287"
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(dpi=100)\n",
    "plt.scatter(X[seed_points:], y[seed_points:], c=jnp.arange(1, len(X[seed_points:])+1),\n",
    "            cmap='viridis', label=\"Sampled points\", zorder=1)\n",
    "cbar = plt.colorbar(label=\"Exploration step\")\n",
    "cbar_ticks = np.arange(2, len(X[seed_points:]) + 1, 2)\n",
    "cbar.set_ticks(cbar_ticks)\n",
    "plt.scatter(X[:seed_points], y[:seed_points], marker='x', s=64,\n",
    "            c='k', label=\"Seed points\", zorder=1)\n",
    "plt.plot(X_test, truefunc, c='k', label='True function', zorder=0)\n",
    "plt.plot(X_test, y_pred, '--', c='red', label='Model reconstruction', zorder=0)\n",
    "\n",
    "plt.fill_between(X_test, y_pred - y_sampled.std((0,1)), y_pred + y_sampled.std((0,1)),\n",
    "                        color='r', alpha=0.2, label=\"Model uncertainty\", zorder=0)\n",
    "plt.xlabel(\"$x$\", fontsize=12)\n",
    "plt.ylabel(\"$y$\", fontsize=12)\n",
    "plt.legend(fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aND1kFMrkQTI"
   },
   "source": [
    "Even though it required more steps (and each step took longer to converge), it is still able to reconstuct the underlying discontinuous function. Note that the vanilla GP won't be able to do so (you can check it by removing the mean_fn and mean_fn_prior arguments from the GP initialization and rerunning the process)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "GP_sGP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
